{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **GPT-fuego (LLM Agent Challenge)** by Nathan Roll"
      ],
      "metadata": {
        "id": "Jm8ESq7Y31CF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Scrape Data Science Notebooks**\n",
        "\n"
      ],
      "metadata": {
        "id": "Td_ngU8x1jGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "FOLDER_PATH = \"/content/drive/Shareddrives/Notebooks\""
      ],
      "metadata": {
        "id": "_GPPCAzOj-mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install -q google-colab-selenium[undetected] # Required to bypass captcha on Kaggle\n",
        "# import google_colab_selenium as gs"
      ],
      "metadata": {
        "id": "Jnl1ReNfmfd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# urls = ['https://www.kaggle.com/code/dczerniawko/fifa19-analysis',\n",
        "#         'https://www.kaggle.com/code/ekami66/detailed-exploratory-data-analysis-with-python',\n",
        "#         'https://www.kaggle.com/code/anokas/data-exploration-analysis',\n",
        "#         'https://www.kaggle.com/code/aeryan/spotify-music-analysis',\n",
        "#         'https://www.kaggle.com/code/anokas/data-analysis-xgboost-starter-0-35460-lb',\n",
        "#         'https://www.kaggle.com/code/edhirif/predict-the-causes-of-wildfires-using-python',\n",
        "#         'https://www.kaggle.com/code/umerkk12/credit-card-predictive-analysis',\n",
        "#         'https://www.kaggle.com/code/dansbecker/wildfire-analysis',\n",
        "#         'https://www.kaggle.com/code/sentdex/first-pass-through-data-w-3d-convnet',\n",
        "#         'https://www.kaggle.com/code/allunia/protein-atlas-exploration-and-baseline',\n",
        "#         'https://www.kaggle.com/code/helgejo/an-interactive-data-science-tutorial',\n",
        "#         'https://www.kaggle.com/code/goyalshalini93/car-price-prediction-linear-regression-rfe',\n",
        "#         'https://www.kaggle.com/code/caesarlupum/ashrae-start-here-a-gentle-introduction',\n",
        "#         'https://www.kaggle.com/code/go1dfish/clear-mask-visualization-and-simple-eda',\n",
        "#         'https://www.kaggle.com/code/thykhuely/mercari-interactive-eda-topic-modelling',\n",
        "#         'https://www.kaggle.com/code/philculliton/nlp-getting-started-tutorial',\n",
        "#         'https://www.kaggle.com/code/kashnitsky/topic-9-part-1-time-series-analysis-in-python',\n",
        "#         'https://www.kaggle.com/code/residentmario/grouping-and-sorting',\n",
        "#         'https://www.kaggle.com/code/ratthachat/aptos-eye-preprocessing-in-diabetic-retinopathy',\n",
        "#         'https://www.kaggle.com/code/codename007/home-credit-complete-eda-feature-importance',\n",
        "#         'https://www.kaggle.com/code/janiobachmann/lending-club-risk-analysis-and-metrics',\n",
        "#         'https://www.kaggle.com/code/etakla/exploring-the-dataset-bivariate-analysis',\n",
        "#         'https://www.kaggle.com/code/nancyalaswad90/analysis-breast-cancer-prediction-dataset',\n",
        "#         'https://www.kaggle.com/code/tirsogomez/banking-dataset-analysis',\n",
        "#         'https://www.kaggle.com/code/carmelgafa/ibm-hr-dataset-analysis-and-prediction',\n",
        "#         'https://www.kaggle.com/code/jneupane12/analysis-of-movielens-dataset-beginner-sanalysis',\n",
        "#         'https://www.kaggle.com/code/volodymyrgavrysh/bank-marketing-campaigns-dataset-analysis',\n",
        "#         'https://www.kaggle.com/code/qusaybtoush1990/airbnb-analysis-dataset',\n",
        "#         'https://www.kaggle.com/code/gsdeepakkumar/e-commerce-dataset-analysis',\n",
        "#         'https://www.kaggle.com/code/lalitharajesh/iris-dataset-exploratory-data-analysis']\n",
        "\n",
        "# for url in tqdm(urls):\n",
        "#   driver = gs.Chrome()\n",
        "#   driver.get(url)\n",
        "#   html = driver.page_source\n",
        "#   driver.quit()\n",
        "\n",
        "#   url_stem = url.split(\"/\")[-1]\n",
        "#   filepath = os.path.join(FOLDER_PATH, url_stem + '.ipynb')\n",
        "\n",
        "#   with open(filepath, \"w\") as f:\n",
        "#     f.write(html)"
      ],
      "metadata": {
        "id": "DQKrAaETdWYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load and Parse Notebooks for Fine-tuning**"
      ],
      "metadata": {
        "id": "KtFNeklbC8LN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "combined_examples = []\n",
        "\n",
        "def format_for_OpenAI(input, code=True):\n",
        "  if code:\n",
        "    return f\"\"\"```python\\n{''.join(input)}\\n```\"\"\"\n",
        "  else:\n",
        "    return ''.join(input)\n",
        "\n",
        "for nb in tqdm(os.listdir(FOLDER_PATH)):\n",
        "  if nb.endswith('.ipynb'):\n",
        "    with open(FOLDER_PATH+'/'+nb) as f:\n",
        "      raw_text = f.read()\n",
        "\n",
        "    cells = json.loads(raw_text)['cells']\n",
        "\n",
        "    examples = []\n",
        "\n",
        "    for i in range(len(cells) - 2):\n",
        "      if cells[i]['cell_type'] == 'markdown' and cells[i+1]['cell_type'] == 'code' and cells[i+1]['cell_type'] == 'markdown':\n",
        "        examples.append((cells[i]['source'], cells[i+1]['source']))\n",
        "      if cells[i]['cell_type'] == 'markdown' and cells[i+1]['cell_type'] == 'code' and cells[i+1]['cell_type'] == 'code':\n",
        "        examples.append((cells[i]['source'], cells[i+1]['source'] + cells[i+2]['source']))\n",
        "\n",
        "    examples = [(format_for_OpenAI(examples[0][1], code=True), format_for_OpenAI(e[0]), format_for_OpenAI(e[1], code=True)) for  e in examples][1:] # add first code cell as context at 0 index for input into prompt\n",
        "    combined_examples.extend(examples)"
      ],
      "metadata": {
        "id": "xvawzPAvm5Nq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1fc76cb-1542-46dc-f04f-6b678d8eb363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31/31 [00:01<00:00, 25.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(combined_examples)\n",
        "\n",
        "validation_set_size = int(len(combined_examples) * 0.15)\n",
        "print(f'val length: {validation_set_size}')\n",
        "\n",
        "validation_set = combined_examples[:validation_set_size]\n",
        "training_set = combined_examples[validation_set_size:]"
      ],
      "metadata": {
        "id": "148RnXSDMVcI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9adf43c0-ae63-48b5-e1b5-64fb288d8773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val length: 93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in ['validation_set', 'training_set']:\n",
        "\n",
        "  with open(f'{s}.jsonl', 'w') as file:\n",
        "    v = eval(s)\n",
        "    for i in range(len(v)):\n",
        "      item = {\n",
        "        \"messages\": [\n",
        "          {\"role\": \"system\", \"content\": \"You are a data science assistant. Please generate python code to answer the user's prompts.\"},\n",
        "          {\"role\": \"user\", \"content\": f\"Context: {v[i][0]}, Prompt: {v[i][1]}\"},\n",
        "          {\"role\": \"assistant\", \"content\": str(v[i][2])}\n",
        "        ]\n",
        "      }\n",
        "\n",
        "      json_line = json.dumps(item)\n",
        "      file.write(json_line + '\\n')"
      ],
      "metadata": {
        "id": "-FlyYSbZMU-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fine-tune GPT 3.5 Turbo**"
      ],
      "metadata": {
        "id": "wHmJ2AiTbPRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai langchain\n",
        "\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "\n",
        "OPENAI_KEY = 'YOUR_KEY_HERE'\n",
        "\n",
        "client = openai.OpenAI(api_key = OPENAI_KEY)"
      ],
      "metadata": {
        "id": "60s04OOOFny6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dev = client.files.create(\n",
        "#   file=open(\"training_set.jsonl\", \"rb\"),\n",
        "#   purpose=\"fine-tune\"\n",
        "# )\n",
        "\n",
        "# valid = client.files.create(\n",
        "#   file=open(\"validation_set.jsonl\", \"rb\"),\n",
        "#   purpose=\"fine-tune\"\n",
        "# )\n",
        "\n",
        "# client.fine_tuning.jobs.create(\n",
        "#   training_file=train_dev.id,\n",
        "#   validation_file=valid.id,\n",
        "#   suffix=\"gpt_fuego_v1\",\n",
        "#   model=\"gpt-3.5-turbo-0125\",\n",
        "#   hyperparameters={\n",
        "#     \"n_epochs\":2\n",
        "#   }\n",
        "# )"
      ],
      "metadata": {
        "id": "2cgs0ExgQzfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Use fine-tuned model**"
      ],
      "metadata": {
        "id": "zXxWRsutjJJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Change the path to point to the correct location of the sqlite database file.\n",
        "db_path = '/content/drive/Shareddrives/Aira- Wildfire/FPA_FOD_20170508.sqlite'\n",
        "\n",
        "# Connect to the sqlite database file and retrieve data as a Pandas dataframe.\n",
        "cnx = sqlite3.connect(db_path)\n",
        "sql = \"SELECT * from Fires\"\n",
        "df = pd.read_sql_query(sql, cnx)"
      ],
      "metadata": {
        "id": "FgFifteuexKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['Shape'], inplace=True)"
      ],
      "metadata": {
        "id": "3ltXS6Z96eD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_sql(prompt):\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-4-turbo-preview\",\n",
        "    temperature = 0,\n",
        "    max_tokens = 50,\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a column selecting agent. You respond only with a tab-separated list of column names that could help answer the prompt and nothing more. Include too many column names instead of too few.\"},\n",
        "      {\"role\": \"user\", \"content\": f\"Prompt: {prompt}, Column Options: OBJECTID, FOD_ID, FPA_ID, SOURCE_SYSTEM_TYPE, SOURCE_SYSTEM,NWCG_REPORTING_AGENCY, NWCG_REPORTING_UNIT_ID,NWCG_REPORTING_UNIT_NAME, SOURCE_REPORTING_UNIT,SOURCE_REPORTING_UNIT_NAME, LOCAL_FIRE_REPORT_ID,LOCAL_INCIDENT_ID, FIRE_CODE, FIRE_NAME,ICS_209_INCIDENT_NUMBER, ICS_209_NAME, MTBS_ID, MTBS_FIRE_NAME,COMPLEX_NAME, FIRE_YEAR, DISCOVERY_DATE, DISCOVERY_DOY,DISCOVERY_TIME, STAT_CAUSE_CODE, STAT_CAUSE_DESCR, CONT_DATE,CONT_DOY, CONT_TIME, FIRE_SIZE, FIRE_SIZE_CLASS, LATITUDE,LONGITUDE, OWNER_CODE, OWNER_DESCR, STATE, COUNTY,FIPS_CODE, FIPS_NAME\"}\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  raw = response.choices[0].message.content\n",
        "  sql = 'SELECT ' + ', '.join(raw.split('\\t')) + ' FROM `translate-413521.wildfire_aira.Fire`'\n",
        "  return sql"
      ],
      "metadata": {
        "id": "UIEESmMnbqcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ret_v(context, prompt):\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"ft:gpt-3.5-turbo-0125:nathannet:gpt-fuego-v1:9BoT37Uh\",\n",
        "    temperature = 0.15,\n",
        "    max_tokens = 500,\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a data science assistant. Please generate python code to answer the user's prompts.\"},\n",
        "      {\"role\": \"user\", \"content\": f\"Context: {context}, Prompt: {prompt} Create a result with only a single line of code using 'df' as an input.\"}\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "iYxJP3CTGe4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def value_pipeline(code):\n",
        "  if '```python' in code:\n",
        "    out = eval(code.split('```python\\n')[1].split('#')[0].split('```')[0].split('\\n')[0])\n",
        "  else:\n",
        "    out = eval(code)\n",
        "  return out"
      ],
      "metadata": {
        "id": "m9cAsNd9o6VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_secure(code):\n",
        "  response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  temperature = 0.1,\n",
        "  max_tokens = 500,\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a code fixing assistant. Respond only in correct python syntax. If the code is already correct, return the exact input.\"},\n",
        "    {\"role\": \"user\", \"content\": code}\n",
        "  ]\n",
        "  )\n",
        "\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "n04OgdDk_AV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "\n",
        "def execute_code_in_pipeline(context):\n",
        "  code = ret_v(context, prompt + ' Do not return a plot.')\n",
        "  code_fixed = fix_secure(code)\n",
        "  # print(code)\n",
        "  outs = {}\n",
        "  for pipe in ['value_pipeline']:\n",
        "    try:\n",
        "      # Dynamically call the pipeline functions with eval\n",
        "      outs[pipe] = eval(f'{pipe}(code_fixed)')\n",
        "    except Exception as e:\n",
        "      outs[pipe] = f'error: {str(e)}'\n",
        "  return outs\n",
        "\n",
        "def multithread_code_output(context):\n",
        "  with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
        "    futures = [executor.submit(execute_code_in_pipeline, context) for _ in range(2)]\n",
        "    results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "WsG-60P54Awn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def package_output(prompt, results):\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-4-turbo-preview\",\n",
        "    temperature = 0,\n",
        "    max_tokens = 1000,\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a data analysis assistant. Please use the outputs to create a succinct, correct answer to the question using only the info you need.\"},\n",
        "      {\"role\": \"user\", \"content\": f'Question: {prompt}, Outputs: {results}.'}\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "gamZJMmlwZ1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Connect to Google Bigquery Wildfire DS**"
      ],
      "metadata": {
        "id": "jGP-5xmzP0eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"key.json\"\n",
        "\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Initialize a BigQuery client\n",
        "client_bq = bigquery.Client()"
      ],
      "metadata": {
        "id": "VM7M-5XuEXG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'What is the average size of a wildfire in California versus Texas?'\n",
        "query = gen_sql(prompt)\n",
        "query_job = client_bq.query(query)\n",
        "df = query_job.to_dataframe()\n",
        "\n",
        "wildfire_context = f'''```python\\n# Connect to the sqlite db file and retrieve data as Pandas data frame.\n",
        "cnx = sqlite3.connect('archive/FPA_FOD_20170508.sqlite')\n",
        "sql = \"select * from fires\"\n",
        "df = pd.read_sql_query(sql, cnx)\\n# Cols = {df.columns}```'''\n",
        "results = multithread_code_output(wildfire_context)\n",
        "\n",
        "o = package_output(prompt, results)"
      ],
      "metadata": {
        "id": "_bgV63E3Dn-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Connect to APIs**"
      ],
      "metadata": {
        "id": "OmBI59CxPw3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install census\n",
        "!pip install us"
      ],
      "metadata": {
        "id": "CWFr3DVZVSbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, json\n",
        "from census import Census\n",
        "from us import states\n",
        "\n",
        "CENSUS_API_KEY = 'YOUR_KEY_HERE'\n",
        "OPEN_WEATHER_API_KEY = 'YOUR_KEY_HERE'\n",
        "\n",
        "\n",
        "weather_base_url = \"http://api.openweathermap.org/data/2.5/weather?\"\n",
        "weather_api_key = OPEN_WEATHER_API_KEY\n",
        "\n",
        "value_keys = {\n",
        "    \"B06011_001E\": \"Median Income\",\n",
        "    \"B18120_002E\": \"Size of labor force\",\n",
        "    \"B18120_012E\": \"Unemployed\"\n",
        "}\n",
        "\n",
        "def get_economy(state_code, year=None):\n",
        "  if year:\n",
        "    c = Census(CENSUS_API_KEY, year=year) # need to re-init each time to avoid sync errors :(\n",
        "  else:\n",
        "    c = Census(CENSUS_API_KEY)\n",
        "\n",
        "  r = c.acs1.get(('NAME', 'B06011_001E', 'B18120_002E', 'B18120_012E'), #combed through ACS tables for fields: https://www.census.gov/programs-surveys/acs/technical-documentation/table-shells.html\n",
        "            {'for': f\"state:{eval(f'states.{state_code}.fips')}\"})[0]\n",
        "\n",
        "  r = {k if k not in value_keys else value_keys[k]: v for k, v in r.items()}\n",
        "  r['Unemployment rate'] = round(r['Unemployed'] / r['Size of labor force'], 4)\n",
        "  return r\n",
        "\n",
        "\n",
        "def kelvin_to_fahrenheit(K):\n",
        "    F = (K * (9/5)) - 459.67\n",
        "    return round(F, 4)\n",
        "\n",
        "def get_weather(place, weather_api_key):\n",
        "\n",
        "  complete_url = weather_base_url + \"appid=\" + weather_api_key + \"&q=\" + place\n",
        "\n",
        "  response = requests.get(complete_url)\n",
        "  x = response.json()\n",
        "\n",
        "  d = {k: kelvin_to_fahrenheit(v) if 200 < v < 400 else v for k, v in x['main'].items()}\n",
        "  d['description_of_weather'] = x['weather'][0]['description']\n",
        "\n",
        "  return d"
      ],
      "metadata": {
        "id": "tIgB_Ms65EkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def API_router(prompt):\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-4-turbo-preview\",\n",
        "    temperature = 0,\n",
        "    max_tokens = 1000,\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are an API assistant which receives a prompt and determines if it involves either a weather API or a census API. The census API only returns information on 'median_income' and 'unemployment_rate' and the weather API only returns the following fields: 'temp' 'feels_like' 'temp_min' 'temp_max' 'pressure' 'humidity' 'description_of_weather'. Respond with either 'census','weather','both', or 'niether' given the prompt. Also include the year, state, and state code if applicable. Do not include any other text. Examples: Prompt: 'Which state has the most whales and how hot was it in 2019?' Response: 'weather 2019 NA NA', Prompt: 'How many ducks are in CA and what was their unemployment rate?' Response: 'census NA California CA'\"},\n",
        "      {\"role\": \"user\", \"content\": f'Prompt: {prompt}'}\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "WmUqnvTfamQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_api(prompt):\n",
        "  try:\n",
        "    routed = API_router(prompt)\n",
        "    print(routed)\n",
        "    api, year, state, state_code = routed.split(' ')\n",
        "    if year == 'NA':\n",
        "      year = None\n",
        "    else:\n",
        "      year = int(year)\n",
        "\n",
        "    if api == 'niether':\n",
        "      pass\n",
        "    elif api == 'census':\n",
        "      r = get_economy(state_code, year)\n",
        "    elif api == 'weather':\n",
        "      r = get_weather(state, weather_api_key)\n",
        "    elif api == 'both':\n",
        "      r = {**get_economy(state, year), **get_weather(state, weather_api_key)}\n",
        "    return r\n",
        "  except ValueError:\n",
        "    return None"
      ],
      "metadata": {
        "id": "Bncm4tGqdCBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_api('How many ducks were in CA in 2005 and what was their max temperature?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi00U6oZkulJ",
        "outputId": "23bcf4a8-7e5c-4b27-f7a6-8d1b4e6e3986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weather 2005 California CA\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'temp': 75.416,\n",
              " 'feels_like': 75.002,\n",
              " 'temp_min': 72.662,\n",
              " 'temp_max': 80.708,\n",
              " 'pressure': 1014,\n",
              " 'humidity': 50,\n",
              " 'description_of_weather': 'clear sky'}"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_weather('California', weather_api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFKltus_6PkV",
        "outputId": "a7a9856a-7fa2-4eca-891c-37ab6b5abd12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'temp': 76.442,\n",
              " 'feels_like': 76.136,\n",
              " 'temp_min': 69.656,\n",
              " 'temp_max': 79.826,\n",
              " 'pressure': 1014,\n",
              " 'humidity': 50,\n",
              " 'desc': 'clear sky'}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_economy('CA', year=2018)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJL_PrWrjNqG",
        "outputId": "b8f3df3d-ae0b-4afe-ff67-3fe7a269216a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NAME': 'California',\n",
              " 'Median Income': 32155.0,\n",
              " 'Size of labor force': 18840089.0,\n",
              " 'Unemployed': 1035440.0,\n",
              " 'state': '06',\n",
              " 'Unemployment rate': 0.055}"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_economy('CA', year=2009)"
      ],
      "metadata": {
        "id": "rPI3UF8ZDh2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89be0b55-40d8-4ad7-f074-206418c3b439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NAME': 'California',\n",
              " 'Median Income': 26130.0,\n",
              " 'Size of labor force': 17718670.0,\n",
              " 'Unemployed': 1966111.0,\n",
              " 'state': '06',\n",
              " 'Unemployment rate': 0.111}"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt ideas**"
      ],
      "metadata": {
        "id": "vKpBHmYTQW0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What was the mean size of a wildfire in California versus Texas in 1999?\n",
        "# Which 6 states had the most wildfires and how many fires did they have?\n",
        "# What was the most common name for a fire in 2008?\n",
        "# What time of day is the most common to discover a fire?\n",
        "# What was the average number of fires per state in 1999?\n",
        "# How many fires between 2002 and 2004 were between 30 and 40 degrees latitude?\n",
        "\n",
        "# Which state had the most wildfires in 2009? What was their median income and max temperature that year?"
      ],
      "metadata": {
        "id": "1OzyALFHQZPG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}